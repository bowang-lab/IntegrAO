{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate simulated Cancer Omics dataset; load the saved model and perform feature importance extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and IntegrAO code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snf\n",
    "from sklearn.cluster import spectral_clustering\n",
    "from sklearn.metrics import v_measure_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import umap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add the parent directory of \"integrao\" to the Python path\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from integrao.dataset import GraphDataset\n",
    "from integrao.main import dist2\n",
    "from integrao.integrater import integrao_integrater, integrao_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "neighbor_size = 20\n",
    "embedding_dims = 64\n",
    "fusing_iteration = 30\n",
    "normalization_factor = 1.0\n",
    "alighment_epochs = 1000\n",
    "beta = 1.0\n",
    "mu = 0.5\n",
    "\n",
    "\n",
    "dataset_name = 'cancer_omics_prediction'\n",
    "cluster_number = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create result dir\n",
    "result_dir = os.path.join(\n",
    "    module_path, \"results/{}\".format(dataset_name)\n",
    ")\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 367)\n",
      "(500, 131)\n",
      "(500, 160)\n",
      "(500, 2)\n",
      "finish loading data!\n"
     ]
    }
   ],
   "source": [
    "testdata_dir = os.path.join(module_path, \"data/omics/\")\n",
    "\n",
    "methyl_ = os.path.join(testdata_dir, \"omics1.txt\")\n",
    "expr_ = os.path.join(testdata_dir, \"omics2.txt\")\n",
    "protein_ = os.path.join(testdata_dir, \"omics3.txt\")\n",
    "truelabel = os.path.join(testdata_dir, \"clusters.txt\")\n",
    "\n",
    "\n",
    "methyl = pd.read_csv(methyl_, index_col=0, delimiter=\"\\t\")\n",
    "expr = pd.read_csv(expr_, index_col=0, delimiter=\"\\t\")\n",
    "protein = pd.read_csv(protein_, index_col=0, delimiter=\"\\t\")\n",
    "truelabel = pd.read_csv(truelabel, index_col=0, delimiter=\"\\t\")\n",
    "\n",
    "methyl = np.transpose(methyl)\n",
    "expr = np.transpose(expr)\n",
    "protein = np.transpose(protein)\n",
    "print(methyl.shape)\n",
    "print(expr.shape)\n",
    "print(protein.shape)\n",
    "print(truelabel.shape)\n",
    "print(\"finish loading data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sub-sample the omics dataset to create an incomplete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.7\n",
    "\n",
    "full_indices = range(len(methyl))\n",
    "unique_indices, common_indices = train_test_split(full_indices, test_size=ratio)\n",
    "\n",
    "w1w2_indices, w3_indices = train_test_split(unique_indices, test_size=0.33)\n",
    "w1_indices, w2_indices = train_test_split(w1w2_indices, test_size=0.5)\n",
    "\n",
    "w1_full_indices = common_indices + w1_indices\n",
    "w2_full_indices = common_indices + w2_indices\n",
    "w3_full_indices = common_indices + w3_indices\n",
    "\n",
    "methyl_temp = methyl.iloc[w1_full_indices]\n",
    "expr_temp = expr.iloc[w2_full_indices]\n",
    "protein_temp = protein.iloc[w3_full_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IntegrAO integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start indexing input expression matrices!\n",
      "Common sample between view0 and view1: 350\n",
      "Common sample between view0 and view2: 350\n",
      "Common sample between view1 and view2: 350\n",
      "Neighbor size: 20\n",
      "Start applying diffusion!\n",
      "Diffusion ends! Times: 4.320159196853638s\n",
      "Starting unsupervised exmbedding extraction!\n",
      "Dataset 0: (400, 367)\n",
      "Dataset 1: (400, 131)\n",
      "Dataset 2: (400, 160)\n",
      "epoch 0: loss 30.35201072692871, align_loss:0.767016\n",
      "epoch 100: loss 20.798744201660156, align_loss:0.159867\n",
      "epoch 200: loss 1.118917465209961, align_loss:0.086430\n",
      "epoch 300: loss 1.1179912090301514, align_loss:0.085688\n",
      "epoch 400: loss 1.116926908493042, align_loss:0.084920\n",
      "epoch 500: loss 1.1157724857330322, align_loss:0.084103\n",
      "epoch 600: loss 1.114532709121704, align_loss:0.083309\n",
      "epoch 700: loss 1.1132341623306274, align_loss:0.082542\n",
      "epoch 800: loss 1.1118789911270142, align_loss:0.081781\n",
      "epoch 900: loss 1.110465407371521, align_loss:0.081072\n",
      "Manifold alignment ends! Times: 7.108541965484619s\n",
      "IntegrAO for clustering union 500 samples NMI score:  0.9564094113101796\n"
     ]
    }
   ],
   "source": [
    "# Initialize integrater\n",
    "integrater = integrao_integrater(\n",
    "    [methyl_temp, expr_temp, protein_temp],\n",
    "    dataset_name,\n",
    "    neighbor_size=neighbor_size,\n",
    "    embedding_dims=embedding_dims,\n",
    "    fusing_iteration=fusing_iteration,\n",
    "    normalization_factor=normalization_factor,\n",
    "    alighment_epochs=alighment_epochs,\n",
    "    beta=beta,\n",
    "    mu=mu,\n",
    ")\n",
    "# data indexing\n",
    "fused_networks = integrater.network_diffusion()\n",
    "embeds_final, S_final, model = integrater.unsupervised_alignment()\n",
    "\n",
    "labels = spectral_clustering(S_final, n_clusters=cluster_number)\n",
    "\n",
    "true_labels = truelabel.sort_values('subjects')['cluster.id'].tolist()\n",
    "\n",
    "score_all = v_measure_score(true_labels, labels)\n",
    "print(\"IntegrAO for clustering union 500 samples NMI score: \", score_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), os.path.join(result_dir, \"model_integrao_unsupervised.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now load the saved model and perform embedding extraction using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start indexing input expression matrices!\n",
      "Common sample between view0 and view1: 500\n",
      "Common sample between view0 and view2: 500\n",
      "Common sample between view1 and view2: 500\n",
      "Neighbor size: 20\n",
      "Start applying diffusion!\n",
      "Diffusion ends! Times: 5.842190980911255s\n"
     ]
    }
   ],
   "source": [
    "# Network fusion for the whole graph; make sure use the integrao_predictor with the same hyperparameters\n",
    "predictor = integrao_predictor(\n",
    "    [methyl, expr, protein],\n",
    "    dataset_name,\n",
    "    modalities_name_list=[\"methyl\", \"expr\", \"protein\"], \n",
    "    neighbor_size=neighbor_size,\n",
    "    embedding_dims=embedding_dims,\n",
    "    fusing_iteration=fusing_iteration,\n",
    "    normalization_factor=normalization_factor,\n",
    "    alighment_epochs=alighment_epochs,\n",
    "    beta=beta,\n",
    "    mu=mu,\n",
    ")\n",
    "# data indexing\n",
    "fused_networks = predictor.network_diffusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-trained model with success.\n",
      "IntegrAO for clustering union 500 samples NMI score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# load model and inference for obtaining the patient embeddings\n",
    "model_path = os.path.join(result_dir, \"model_integrao_unsupervised.pth\")\n",
    "final_embedding_df, S_final = predictor.inference_unsupervised(model_path, new_datasets=[methyl, expr, protein], modalities_names=[\"methyl\", \"expr\", \"protein\"])\n",
    "\n",
    "labels = spectral_clustering(S_final, n_clusters=cluster_number)\n",
    "\n",
    "true_labels = truelabel.sort_values('subjects')['cluster.id'].tolist()\n",
    "\n",
    "score_all = v_measure_score(true_labels, labels)\n",
    "print(\"IntegrAO for clustering union 500 samples NMI score: \", score_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "integrAO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
